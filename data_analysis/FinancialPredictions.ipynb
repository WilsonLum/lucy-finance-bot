{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinancialPredictions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5do3GPzVb4mS",
        "W0Qah40Bb8Km"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgWa7Wg5rTAR",
        "colab_type": "code",
        "outputId": "5abd355b-89e2-44e0-f787-937e6cbd6e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Ignore this cell\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/datasets/finance_social/Dataset')\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['StockTwits',\n",
              " 'Twitter',\n",
              " 'ReutersNews',\n",
              " 'twiters_label_data_Feb-02-2020.xlsx',\n",
              " 'Stocktwits Label Model training with twiter data.ipynb',\n",
              " 'github data',\n",
              " 'INTENTS and UTTERANCE.yaml',\n",
              " 'Keras_FNN_model.json',\n",
              " 'finalized_Keras_model.sav',\n",
              " 'Stocktwits Label Model training with twiter data - Final.ipynb',\n",
              " 'Keras KNN test result 1.PNG',\n",
              " 'Keras KNN test result 1a.PNG',\n",
              " 'finalized_Keras_model_1.sav',\n",
              " 'Keras_FNN_model_1.json',\n",
              " 'Keras_FNN_model.h5',\n",
              " 'Stocktwits Keras FNN for sentiment analysis-Final.ipynb',\n",
              " 'finalized_SVM_model.sav',\n",
              " 'chatbot_training.yaml',\n",
              " 'stocktwits_sentiment_subjectivity_Feb-01-2020_p.xlsx',\n",
              " 'twitter_ul.npy',\n",
              " 'FinancialPrediction',\n",
              " 'variant_filterer.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQqkE8Dgr16D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import date,timedelta,datetime\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics \n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwlzTtw-rfJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stocktwits_file=Path('StockTwits/stocktwits_labelled_v3.pkl')\n",
        "twitter_file = Path('Twitter/twitter_labelled_v3.pkl')\n",
        "finpred_file = Path('FinancialPrediction/finpred_features.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXb--eSrrwfy",
        "colab_type": "code",
        "outputId": "7fc9d074-d009-4937-ea16-a1129965e477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data = pd.read_pickle(stocktwits_file)\n",
        "print(data.shape)\n",
        "print(data.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128140, 38)\n",
            "Index(['id', 'body', 'created_at', 'user', 'source', 'symbols',\n",
            "       'mentioned_users', 'entities', 'filters', 'conversation', 'likes',\n",
            "       'links', 'reshare_message', 'reshares', 'structurable', 'ticker',\n",
            "       'user_followers', 'user_following', 'user_join_date', 'user_ideas',\n",
            "       'user_identity', 'user_like_count', 'user_official',\n",
            "       'user_wtchlst_count', 'username', 'sentiment', 'num_likes',\n",
            "       'num_reshares', 'num_replies', 'day_counts', 'raw_body', 'char_length',\n",
            "       'bearish_score', 'bullish_score', 'sentiment_pred', '1_day_return',\n",
            "       '3_day_return', '5_day_return'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igadaKfhtxqQ",
        "colab_type": "text"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so70el6Xt3Ts",
        "colab_type": "text"
      },
      "source": [
        "## Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrMD_B9Ur993",
        "colab_type": "code",
        "outputId": "fbcf3a86-6641-4bdb-dfce-eea02202be08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "plt.figure(figsize=(16,7))\n",
        "data.isna().mean().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1127278cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAHxCAYAAAB3fQH3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neq6kNJct858",
        "colab_type": "text"
      },
      "source": [
        "## Training data size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bImDh-m9sGXK",
        "colab_type": "code",
        "outputId": "f75816a8-797e-47f9-9b4c-a672366cb687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "print('Number of unique ticker and date pairs: ', data.groupby(['created_at','ticker'])['id'].count().shape[0])\n",
        "data.groupby(['created_at','ticker'])['id'].count().plot(kind='hist')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique ticker and date pairs:  1103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f107d79beb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAD4CAYAAADPccAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSV0ZLJKkrv4/HuvA0AvcZGIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VoV2OozZRpF",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oS97IZONxTW",
        "colab_type": "code",
        "outputId": "7e05468e-94f4-48ac-a1c0-864af0b365ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'body', 'created_at', 'user', 'source', 'symbols',\n",
              "       'mentioned_users', 'entities', 'filters', 'conversation', 'likes',\n",
              "       'links', 'reshare_message', 'reshares', 'structurable', 'ticker',\n",
              "       'user_followers', 'user_following', 'user_join_date', 'user_ideas',\n",
              "       'user_identity', 'user_like_count', 'user_official',\n",
              "       'user_wtchlst_count', 'username', 'sentiment', 'num_likes',\n",
              "       'num_reshares', 'num_replies', 'day_counts', 'raw_body', 'char_length',\n",
              "       'bearish_score', 'bullish_score', 'sentiment_pred', '1_day_return',\n",
              "       '3_day_return', '5_day_return'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z6M-R2ANI4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_cols = ['id', 'created_at', 'ticker']\n",
        "feature_cols = ['num_likes','num_reshares', 'num_replies', 'bearish_score', 'bullish_score']\n",
        "label_cols = ['1_day_return','3_day_return', '5_day_return', ]\n",
        "maybe_cols = ['links','user_followers', 'user_following', 'user_join_date','user_identity', 'user_like_count', 'user_official',\n",
        "              'user_wtchlst_count', 'user_like_count', 'day_counts']\n",
        "irrelevant_cols = ['user', 'source', 'symbols','mentioned_users', 'entities', 'filters', 'conversation', 'likes', 'reshare_message','reshares',\n",
        "                   'structurable', 'user_ideas','username', 'sentiment', 'raw_body', 'body','char_length','sentiment_pred']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiBR8YkgPk5a",
        "colab_type": "code",
        "outputId": "7179e8a1-2d7d-4643-da7b-f2722cc2fbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "identified_cols = set(id_cols + feature_cols + label_cols + maybe_cols + irrelevant_cols)\n",
        "[col for col in data.columns if col not in identified_cols]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAwPQ8J4vHne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_group = data.loc[:, id_cols + feature_cols + label_cols].groupby(['created_at', 'ticker'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgfSsKF3U9a6",
        "colab_type": "code",
        "outputId": "28fd7b84-355d-4cf7-cd94-32e4e2493cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels_df = data_group[label_cols].mean()\n",
        "\n",
        "feature_df_sum = data_group[feature_cols].sum()\n",
        "feature_df_sum.columns = [f + '_sum' for f in feature_cols]\n",
        "\n",
        "feature_df_mean = data_group[feature_cols].mean()\n",
        "feature_df_mean.columns = [f + '_mean' for f in feature_cols]\n",
        "\n",
        "feature_data = pd.concat([feature_df_sum, feature_df_mean, labels_df], axis=1)\n",
        "feature_data.to_pickle(finpred_file)\n",
        "feature_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1103, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeB9WWkcvGNx",
        "colab_type": "text"
      },
      "source": [
        "# Predictive Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGzxQkTEbFaB",
        "colab_type": "code",
        "outputId": "b721cb52-8cce-4657-8523-92314eb531c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "feature_data = pd.read_pickle(finpred_file)\n",
        "feature_data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>num_likes_sum</th>\n",
              "      <th>num_reshares_sum</th>\n",
              "      <th>num_replies_sum</th>\n",
              "      <th>bearish_score_sum</th>\n",
              "      <th>bullish_score_sum</th>\n",
              "      <th>num_likes_mean</th>\n",
              "      <th>num_reshares_mean</th>\n",
              "      <th>num_replies_mean</th>\n",
              "      <th>bearish_score_mean</th>\n",
              "      <th>bullish_score_mean</th>\n",
              "      <th>1_day_return</th>\n",
              "      <th>3_day_return</th>\n",
              "      <th>5_day_return</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>created_at</th>\n",
              "      <th>ticker</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-05-20</th>\n",
              "      <th>VIX</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>19.185993</td>\n",
              "      <td>34.814007</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.814815</td>\n",
              "      <td>0.355296</td>\n",
              "      <td>0.644704</td>\n",
              "      <td>-0.049601</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.010929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-21</th>\n",
              "      <th>VIX</th>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>9.948185</td>\n",
              "      <td>20.051815</td>\n",
              "      <td>1.533333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>0.331606</td>\n",
              "      <td>0.668394</td>\n",
              "      <td>-0.011057</td>\n",
              "      <td>0.033171</td>\n",
              "      <td>0.084918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-22</th>\n",
              "      <th>VIX</th>\n",
              "      <td>86</td>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>18.800280</td>\n",
              "      <td>33.199722</td>\n",
              "      <td>1.653846</td>\n",
              "      <td>0.057692</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.361544</td>\n",
              "      <td>0.638456</td>\n",
              "      <td>0.066190</td>\n",
              "      <td>0.075581</td>\n",
              "      <td>0.074240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-23</th>\n",
              "      <th>VIX</th>\n",
              "      <td>103</td>\n",
              "      <td>4</td>\n",
              "      <td>78</td>\n",
              "      <td>33.612217</td>\n",
              "      <td>39.387783</td>\n",
              "      <td>1.410959</td>\n",
              "      <td>0.054795</td>\n",
              "      <td>1.068493</td>\n",
              "      <td>0.460441</td>\n",
              "      <td>0.539559</td>\n",
              "      <td>-0.020134</td>\n",
              "      <td>0.028943</td>\n",
              "      <td>0.051594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-24</th>\n",
              "      <th>VIX</th>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>58</td>\n",
              "      <td>11.992064</td>\n",
              "      <td>30.007936</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>1.380952</td>\n",
              "      <td>0.285525</td>\n",
              "      <td>0.714475</td>\n",
              "      <td>0.029538</td>\n",
              "      <td>0.028253</td>\n",
              "      <td>0.080051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   num_likes_sum  num_reshares_sum  ...  3_day_return  5_day_return\n",
              "created_at ticker                                   ...                            \n",
              "2019-05-20 VIX                45                 1  ...      0.002102      0.010929\n",
              "2019-05-21 VIX                46                 1  ...      0.033171      0.084918\n",
              "2019-05-22 VIX                86                 3  ...      0.075581      0.074240\n",
              "2019-05-23 VIX               103                 4  ...      0.028943      0.051594\n",
              "2019-05-24 VIX                48                 2  ...      0.028253      0.080051\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvupuPS0dE4x",
        "colab_type": "code",
        "outputId": "b9205b0e-6d07-4d0e-aeaa-abf7bf3a17d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "sentiment_cols = ['bearish_score_mean', 'bearish_score_sum', 'bullish_score_mean','bullish_score_sum']\n",
        "# in_cols = feature_df_sum.columns.union(feature_df_mean.columns)\n",
        "in_cols = sentiment_cols\n",
        "out_cols = label_cols[:]\n",
        "X = feature_data.loc[:, in_cols]\n",
        "y = feature_data[out_cols]\n",
        "y_stratify = y.reset_index()['ticker']\n",
        "\n",
        "xscaler = MinMaxScaler()\n",
        "yscaler = MinMaxScaler((-1,1))\n",
        "X.loc[:] = xscaler.fit_transform(X)\n",
        "y.loc[:] = yscaler.fit_transform(y)\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y_stratify)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlE66qLRsNIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "817ec26f-1e01-428b-bf2f-ddbd77167096"
      },
      "source": [
        "in_cols"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bearish_score_mean',\n",
              " 'bearish_score_sum',\n",
              " 'bullish_score_mean',\n",
              " 'bullish_score_sum']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN8I04-74phk",
        "colab_type": "code",
        "outputId": "304da6d0-7e22-4865-9d52-295ce9525876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "returns, bins = pd.cut(labels_df['1_day_return'], 5, retbins=True)\n",
        "returns.value_counts().plot(kind='bar')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f107d2b4160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFJCAYAAABgsG0jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIyIqlHCPiKjQ/wcutewm35N/8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfmNWKgMrL0_",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network(Keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UooqKu1rY_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_dim, output_dim):\n",
        "\t# create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(3, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
        "  model.add(Dense(5, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
        "  model.add(Dense(output_dim, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "  optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJtMT6NEwlvA",
        "colab_type": "code",
        "outputId": "9998e856-a3a7-4b39-db7b-20f467c04018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = build_model(X.shape[1], y_train.shape[1])\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 15        \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 20        \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 18        \n",
            "=================================================================\n",
            "Total params: 53\n",
            "Trainable params: 53\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owCRtuDeyLRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"weights-improvement-{val_loss:.2f}-{epoch:02d}.hdf5\"\n",
        "checkpoint      = ModelCheckpoint('/content/drive/My Drive/logs/finpred_5_/'+filepath, \n",
        "                                  monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min')\n",
        "csv_logger      = CSVLogger(\"variant_filterer\" +'.csv')\n",
        "callbacks = [checkpoint,csv_logger]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoLZgp6wwyfb",
        "colab_type": "code",
        "outputId": "7134d925-f630-4820-8625-b75eb56769eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000, verbose=1, callbacks=callbacks, validation_split=0.15)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.1112\n",
            "Epoch 00001: val_loss improved from inf to 0.09733, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.10-01.hdf5\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1137 - val_loss: 0.0973\n",
            "Epoch 2/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0824\n",
            "Epoch 00002: val_loss improved from 0.09733 to 0.08216, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.08-02.hdf5\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0980 - val_loss: 0.0822\n",
            "Epoch 3/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0710\n",
            "Epoch 00003: val_loss improved from 0.08216 to 0.06604, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.07-03.hdf5\n",
            "25/25 [==============================] - 0s 15ms/step - loss: 0.0806 - val_loss: 0.0660\n",
            "Epoch 4/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0881\n",
            "Epoch 00004: val_loss improved from 0.06604 to 0.05038, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.05-04.hdf5\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0628 - val_loss: 0.0504\n",
            "Epoch 5/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0653\n",
            "Epoch 00005: val_loss improved from 0.05038 to 0.03902, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.04-05.hdf5\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0468 - val_loss: 0.0390\n",
            "Epoch 6/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00006: val_loss improved from 0.03902 to 0.03458, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-06.hdf5\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.0365 - val_loss: 0.0346\n",
            "Epoch 7/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00007: val_loss improved from 0.03458 to 0.03423, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-07.hdf5\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0329 - val_loss: 0.0342\n",
            "Epoch 8/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00008: val_loss improved from 0.03423 to 0.03422, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-08.hdf5\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0323 - val_loss: 0.0342\n",
            "Epoch 9/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 00009: val_loss improved from 0.03422 to 0.03415, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-09.hdf5\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 10/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0306\n",
            "Epoch 00010: val_loss did not improve from 0.03415\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 11/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00011: val_loss improved from 0.03415 to 0.03408, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-11.hdf5\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 12/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00012: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 13/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 00013: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 14/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0337\n",
            "Epoch 00014: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0341\n",
            "Epoch 15/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00015: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 16/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0149\n",
            "Epoch 00016: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 17/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0469\n",
            "Epoch 00017: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 18/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0263\n",
            "Epoch 00018: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 19/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 00019: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 20/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0223\n",
            "Epoch 00020: val_loss did not improve from 0.03408\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0342\n",
            "Epoch 21/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00021: val_loss improved from 0.03408 to 0.03404, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-21.hdf5\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 22/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0508\n",
            "Epoch 00022: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0342\n",
            "Epoch 23/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0357\n",
            "Epoch 00023: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 24/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00024: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 25/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00025: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0342\n",
            "Epoch 26/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0343\n",
            "Epoch 00026: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 27/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0306\n",
            "Epoch 00027: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0342\n",
            "Epoch 28/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0169\n",
            "Epoch 00028: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 29/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0352\n",
            "Epoch 00029: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 30/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 00030: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 31/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00031: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 32/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0324\n",
            "Epoch 00032: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0342\n",
            "Epoch 33/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00033: val_loss did not improve from 0.03404\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 34/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0223\n",
            "Epoch 00034: val_loss improved from 0.03404 to 0.03402, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-34.hdf5\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 35/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0401\n",
            "Epoch 00035: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 36/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00036: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 37/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0498\n",
            "Epoch 00037: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 38/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00038: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 39/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0412\n",
            "Epoch 00039: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0341\n",
            "Epoch 40/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00040: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0341\n",
            "Epoch 41/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00041: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 42/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 00042: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 43/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00043: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 44/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0347\n",
            "Epoch 00044: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 45/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00045: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 46/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0465\n",
            "Epoch 00046: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 47/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00047: val_loss did not improve from 0.03402\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0342\n",
            "Epoch 48/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0498\n",
            "Epoch 00048: val_loss improved from 0.03402 to 0.03400, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-48.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 49/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0344\n",
            "Epoch 00049: val_loss did not improve from 0.03400\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 50/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0660\n",
            "Epoch 00050: val_loss did not improve from 0.03400\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 51/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0593\n",
            "Epoch 00051: val_loss improved from 0.03400 to 0.03397, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-51.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 52/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00052: val_loss did not improve from 0.03397\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 53/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0478\n",
            "Epoch 00053: val_loss did not improve from 0.03397\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 54/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0334\n",
            "Epoch 00054: val_loss did not improve from 0.03397\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 55/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0337\n",
            "Epoch 00055: val_loss improved from 0.03397 to 0.03390, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-55.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 56/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0225\n",
            "Epoch 00056: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0342\n",
            "Epoch 57/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00057: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 58/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0346\n",
            "Epoch 00058: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 59/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0251\n",
            "Epoch 00059: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 60/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0403\n",
            "Epoch 00060: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 61/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0415\n",
            "Epoch 00061: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 62/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0418\n",
            "Epoch 00062: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 63/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0505\n",
            "Epoch 00063: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0343\n",
            "Epoch 64/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00064: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 65/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0342\n",
            "Epoch 00065: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 66/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0240\n",
            "Epoch 00066: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 67/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 00067: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0340\n",
            "Epoch 68/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00068: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 69/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0326\n",
            "Epoch 00069: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 70/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 00070: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 71/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0257\n",
            "Epoch 00071: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 72/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0353\n",
            "Epoch 00072: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 73/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0381\n",
            "Epoch 00073: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0341\n",
            "Epoch 74/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00074: val_loss improved from 0.03390 to 0.03390, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-74.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 75/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00075: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 76/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00076: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 77/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00077: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 78/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0252\n",
            "Epoch 00078: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 79/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00079: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 80/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0281\n",
            "Epoch 00080: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 81/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0319\n",
            "Epoch 00081: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 82/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0285\n",
            "Epoch 00082: val_loss did not improve from 0.03390\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 83/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00083: val_loss improved from 0.03390 to 0.03387, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-83.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 84/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0303\n",
            "Epoch 00084: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 85/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0348\n",
            "Epoch 00085: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 86/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 00086: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 87/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0132\n",
            "Epoch 00087: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 88/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00088: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0340\n",
            "Epoch 89/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0445\n",
            "Epoch 00089: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 90/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0240\n",
            "Epoch 00090: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 91/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0540\n",
            "Epoch 00091: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 92/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00092: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 93/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 00093: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 94/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00094: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 95/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00095: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 96/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00096: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 97/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0126\n",
            "Epoch 00097: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 98/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00098: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 99/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00099: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 100/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0212\n",
            "Epoch 00100: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 101/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00101: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 102/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0322\n",
            "Epoch 00102: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 103/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0163\n",
            "Epoch 00103: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 104/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00104: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 105/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0289\n",
            "Epoch 00105: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 106/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00106: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 107/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0203\n",
            "Epoch 00107: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0341\n",
            "Epoch 108/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0556\n",
            "Epoch 00108: val_loss did not improve from 0.03387\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 109/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 00109: val_loss improved from 0.03387 to 0.03381, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-109.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 110/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0461\n",
            "Epoch 00110: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 111/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00111: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 112/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0619\n",
            "Epoch 00112: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 113/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0140\n",
            "Epoch 00113: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 114/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0470\n",
            "Epoch 00114: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 115/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00115: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 116/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0389\n",
            "Epoch 00116: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 117/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0326\n",
            "Epoch 00117: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 118/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00118: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 119/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0343\n",
            "Epoch 00119: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 120/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00120: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 121/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00121: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0341\n",
            "Epoch 122/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0323\n",
            "Epoch 00122: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 123/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0356\n",
            "Epoch 00123: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 124/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0373\n",
            "Epoch 00124: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 125/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0366\n",
            "Epoch 00125: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 126/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00126: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 127/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0378\n",
            "Epoch 00127: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 128/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00128: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 129/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0496\n",
            "Epoch 00129: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0342\n",
            "Epoch 130/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0171\n",
            "Epoch 00130: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 131/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00131: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 132/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00132: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 133/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00133: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 134/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00134: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 135/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0428\n",
            "Epoch 00135: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 136/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0457\n",
            "Epoch 00136: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 137/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0369\n",
            "Epoch 00137: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 138/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00138: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 139/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0421\n",
            "Epoch 00139: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 140/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00140: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 141/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0408\n",
            "Epoch 00141: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 142/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00142: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 143/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00143: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 144/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0531\n",
            "Epoch 00144: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 145/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 00145: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 146/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0166\n",
            "Epoch 00146: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 147/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00147: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 148/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0636\n",
            "Epoch 00148: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 149/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0112\n",
            "Epoch 00149: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 150/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0266\n",
            "Epoch 00150: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 151/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00151: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 152/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0240\n",
            "Epoch 00152: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 153/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0120\n",
            "Epoch 00153: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 154/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00154: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 155/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00155: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 156/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00156: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 157/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0362\n",
            "Epoch 00157: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 158/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0213\n",
            "Epoch 00158: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 159/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0360\n",
            "Epoch 00159: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 160/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00160: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 161/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0510\n",
            "Epoch 00161: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 162/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0383\n",
            "Epoch 00162: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 163/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0401\n",
            "Epoch 00163: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 164/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0192\n",
            "Epoch 00164: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 165/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00165: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 166/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0276\n",
            "Epoch 00166: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 167/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0413\n",
            "Epoch 00167: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0341\n",
            "Epoch 168/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0132\n",
            "Epoch 00168: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 169/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00169: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 170/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00170: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 171/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0362\n",
            "Epoch 00171: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 172/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0114\n",
            "Epoch 00172: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 173/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0363\n",
            "Epoch 00173: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 174/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0366\n",
            "Epoch 00174: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 175/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0268\n",
            "Epoch 00175: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 176/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0263\n",
            "Epoch 00176: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 177/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0512\n",
            "Epoch 00177: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 178/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00178: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 179/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0281\n",
            "Epoch 00179: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 180/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0644\n",
            "Epoch 00180: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 181/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0333\n",
            "Epoch 00181: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 182/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00182: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 183/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0095\n",
            "Epoch 00183: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 184/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00184: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0341\n",
            "Epoch 185/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0603\n",
            "Epoch 00185: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 186/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00186: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 187/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00187: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 188/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0290\n",
            "Epoch 00188: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 189/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00189: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 190/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0330\n",
            "Epoch 00190: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 191/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0253\n",
            "Epoch 00191: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 192/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0375\n",
            "Epoch 00192: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 193/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0306\n",
            "Epoch 00193: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 194/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00194: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 195/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00195: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 196/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0365\n",
            "Epoch 00196: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 197/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0352\n",
            "Epoch 00197: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 198/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 00198: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 199/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0264\n",
            "Epoch 00199: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 200/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0314\n",
            "Epoch 00200: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 201/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0131\n",
            "Epoch 00201: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 202/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0396\n",
            "Epoch 00202: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 203/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00203: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 204/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 00204: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 205/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0393\n",
            "Epoch 00205: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 206/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0118\n",
            "Epoch 00206: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 207/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0326\n",
            "Epoch 00207: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 208/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00208: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 209/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0392\n",
            "Epoch 00209: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 210/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00210: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 211/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00211: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 212/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00212: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0341\n",
            "Epoch 213/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0402\n",
            "Epoch 00213: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 214/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0136\n",
            "Epoch 00214: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 215/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0132\n",
            "Epoch 00215: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 216/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0234\n",
            "Epoch 00216: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 217/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0306\n",
            "Epoch 00217: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 218/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0509\n",
            "Epoch 00218: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 219/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00219: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 220/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0369\n",
            "Epoch 00220: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 221/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0334\n",
            "Epoch 00221: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 222/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0317\n",
            "Epoch 00222: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 223/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 00223: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 224/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0390\n",
            "Epoch 00224: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 225/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00225: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 226/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00226: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0342\n",
            "Epoch 227/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00227: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 228/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0245\n",
            "Epoch 00228: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 229/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0297\n",
            "Epoch 00229: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 230/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0215\n",
            "Epoch 00230: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 231/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0298\n",
            "Epoch 00231: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 232/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0225\n",
            "Epoch 00232: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 233/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0554\n",
            "Epoch 00233: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 234/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0309\n",
            "Epoch 00234: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 235/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00235: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 236/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00236: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 237/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0324\n",
            "Epoch 00237: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 238/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0375\n",
            "Epoch 00238: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 239/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00239: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 240/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0279\n",
            "Epoch 00240: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 241/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0548\n",
            "Epoch 00241: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 242/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00242: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 243/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0542\n",
            "Epoch 00243: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 244/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00244: val_loss did not improve from 0.03381\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 245/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0345\n",
            "Epoch 00245: val_loss improved from 0.03381 to 0.03379, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-245.hdf5\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 246/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0107\n",
            "Epoch 00246: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 247/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0598\n",
            "Epoch 00247: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 248/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00248: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 249/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0314\n",
            "Epoch 00249: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 250/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0462\n",
            "Epoch 00250: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 251/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0292\n",
            "Epoch 00251: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 252/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0290\n",
            "Epoch 00252: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 253/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0263\n",
            "Epoch 00253: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 254/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00254: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 255/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00255: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 256/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00256: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 257/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0261\n",
            "Epoch 00257: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 258/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0494\n",
            "Epoch 00258: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 259/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0275\n",
            "Epoch 00259: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 260/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0599\n",
            "Epoch 00260: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 261/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00261: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 262/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0541\n",
            "Epoch 00262: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 263/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0236\n",
            "Epoch 00263: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 264/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0333\n",
            "Epoch 00264: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 265/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0401\n",
            "Epoch 00265: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 266/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0305\n",
            "Epoch 00266: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 267/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0259\n",
            "Epoch 00267: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 268/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0211\n",
            "Epoch 00268: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 269/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0389\n",
            "Epoch 00269: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 270/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00270: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 271/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0339\n",
            "Epoch 00271: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 272/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0156\n",
            "Epoch 00272: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 273/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00273: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 274/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00274: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 275/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0607\n",
            "Epoch 00275: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 276/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0222\n",
            "Epoch 00276: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 277/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0473\n",
            "Epoch 00277: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 278/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0293\n",
            "Epoch 00278: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 279/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0307\n",
            "Epoch 00279: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 280/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0523\n",
            "Epoch 00280: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 281/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00281: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 282/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00282: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 283/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00283: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 284/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0277\n",
            "Epoch 00284: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 285/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0414\n",
            "Epoch 00285: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 286/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0373\n",
            "Epoch 00286: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 287/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0373\n",
            "Epoch 00287: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 288/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00288: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 289/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0411\n",
            "Epoch 00289: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 290/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0298\n",
            "Epoch 00290: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 291/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00291: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 292/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00292: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 293/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0434\n",
            "Epoch 00293: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 294/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0381\n",
            "Epoch 00294: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 295/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00295: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 296/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00296: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 297/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00297: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 298/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0155\n",
            "Epoch 00298: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 299/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0261\n",
            "Epoch 00299: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 300/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0220\n",
            "Epoch 00300: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 301/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0446\n",
            "Epoch 00301: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 302/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0345\n",
            "Epoch 00302: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 303/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0243\n",
            "Epoch 00303: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 304/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0283\n",
            "Epoch 00304: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 305/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0462\n",
            "Epoch 00305: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 306/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0585\n",
            "Epoch 00306: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 307/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0261\n",
            "Epoch 00307: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 308/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0257\n",
            "Epoch 00308: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 309/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 00309: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 310/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0157\n",
            "Epoch 00310: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 311/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 00311: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 312/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0533\n",
            "Epoch 00312: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 313/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0266\n",
            "Epoch 00313: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 314/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00314: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 315/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0326\n",
            "Epoch 00315: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 316/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0292\n",
            "Epoch 00316: val_loss did not improve from 0.03379\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 317/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0259\n",
            "Epoch 00317: val_loss improved from 0.03379 to 0.03376, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-317.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 318/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00318: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 319/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0585\n",
            "Epoch 00319: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 320/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0310\n",
            "Epoch 00320: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 321/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00321: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 322/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0109\n",
            "Epoch 00322: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 323/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00323: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 324/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00324: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 325/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 00325: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 326/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0264\n",
            "Epoch 00326: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 327/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0378\n",
            "Epoch 00327: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 328/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00328: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 329/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0340\n",
            "Epoch 00329: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 330/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0380\n",
            "Epoch 00330: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 331/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00331: val_loss did not improve from 0.03376\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 332/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0196\n",
            "Epoch 00332: val_loss improved from 0.03376 to 0.03375, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-332.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 333/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0192\n",
            "Epoch 00333: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 334/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0387\n",
            "Epoch 00334: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 335/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00335: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 336/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0337\n",
            "Epoch 00336: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 337/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0366\n",
            "Epoch 00337: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 338/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0412\n",
            "Epoch 00338: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 339/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0290\n",
            "Epoch 00339: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 340/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0529\n",
            "Epoch 00340: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 341/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0471\n",
            "Epoch 00341: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 342/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00342: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 343/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00343: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 344/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 00344: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 345/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0240\n",
            "Epoch 00345: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 346/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0213\n",
            "Epoch 00346: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 347/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0403\n",
            "Epoch 00347: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 348/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0362\n",
            "Epoch 00348: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 349/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 00349: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 350/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00350: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 351/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00351: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 352/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00352: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 353/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0300\n",
            "Epoch 00353: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 354/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0256\n",
            "Epoch 00354: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 355/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0314\n",
            "Epoch 00355: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 356/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00356: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 357/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0447\n",
            "Epoch 00357: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 358/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0808\n",
            "Epoch 00358: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 359/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0231\n",
            "Epoch 00359: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 360/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00360: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 361/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0311\n",
            "Epoch 00361: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 362/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0488\n",
            "Epoch 00362: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 363/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0280\n",
            "Epoch 00363: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 364/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0388\n",
            "Epoch 00364: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 365/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00365: val_loss did not improve from 0.03375\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 366/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00366: val_loss improved from 0.03375 to 0.03373, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-366.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 367/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0372\n",
            "Epoch 00367: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 368/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00368: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 369/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0364\n",
            "Epoch 00369: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 370/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00370: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 371/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00371: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 372/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0283\n",
            "Epoch 00372: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 373/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0317\n",
            "Epoch 00373: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 374/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00374: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 375/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00375: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 376/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00376: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 377/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00377: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 378/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0432\n",
            "Epoch 00378: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 379/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00379: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 380/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0394\n",
            "Epoch 00380: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 381/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0327\n",
            "Epoch 00381: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 382/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0223\n",
            "Epoch 00382: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 383/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0333\n",
            "Epoch 00383: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 384/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00384: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 385/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 00385: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 386/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00386: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 387/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0375\n",
            "Epoch 00387: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 388/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0294\n",
            "Epoch 00388: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 389/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00389: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 390/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0508\n",
            "Epoch 00390: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 391/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00391: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 392/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00392: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 393/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0323\n",
            "Epoch 00393: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 394/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0163\n",
            "Epoch 00394: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0340\n",
            "Epoch 395/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0150\n",
            "Epoch 00395: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 396/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0383\n",
            "Epoch 00396: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 397/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0303\n",
            "Epoch 00397: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 398/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0418\n",
            "Epoch 00398: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 399/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0487\n",
            "Epoch 00399: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 400/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00400: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 401/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0469\n",
            "Epoch 00401: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 402/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0308\n",
            "Epoch 00402: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 403/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00403: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 404/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0270\n",
            "Epoch 00404: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 405/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0330\n",
            "Epoch 00405: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 406/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00406: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 407/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00407: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 408/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0383\n",
            "Epoch 00408: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 409/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00409: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 410/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0404\n",
            "Epoch 00410: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 411/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0256\n",
            "Epoch 00411: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 412/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00412: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 413/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0784\n",
            "Epoch 00413: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 414/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00414: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 415/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0290\n",
            "Epoch 00415: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 416/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00416: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 417/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0221\n",
            "Epoch 00417: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 418/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0367\n",
            "Epoch 00418: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 419/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0300\n",
            "Epoch 00419: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 420/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0382\n",
            "Epoch 00420: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 421/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0521\n",
            "Epoch 00421: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 422/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0277\n",
            "Epoch 00422: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 423/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 00423: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 424/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0520\n",
            "Epoch 00424: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 425/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0266\n",
            "Epoch 00425: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 426/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0432\n",
            "Epoch 00426: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 427/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00427: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 428/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00428: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 429/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0409\n",
            "Epoch 00429: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 430/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0416\n",
            "Epoch 00430: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 431/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00431: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 432/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0340\n",
            "Epoch 00432: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 433/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00433: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 434/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0527\n",
            "Epoch 00434: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 435/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00435: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 436/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00436: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 437/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00437: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 438/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0270\n",
            "Epoch 00438: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 439/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0311\n",
            "Epoch 00439: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 440/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0752\n",
            "Epoch 00440: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 441/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00441: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 442/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00442: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 443/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0162\n",
            "Epoch 00443: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 444/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 00444: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 445/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00445: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 446/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0352\n",
            "Epoch 00446: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 447/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0317\n",
            "Epoch 00447: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 448/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0153\n",
            "Epoch 00448: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 449/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0294\n",
            "Epoch 00449: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 450/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0141\n",
            "Epoch 00450: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 451/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0271\n",
            "Epoch 00451: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 452/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0355\n",
            "Epoch 00452: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 453/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0247\n",
            "Epoch 00453: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 454/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0361\n",
            "Epoch 00454: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 455/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 00455: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 456/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0213\n",
            "Epoch 00456: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 457/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0324\n",
            "Epoch 00457: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 458/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0240\n",
            "Epoch 00458: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 459/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0318\n",
            "Epoch 00459: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 460/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0285\n",
            "Epoch 00460: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 461/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0427\n",
            "Epoch 00461: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 462/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0535\n",
            "Epoch 00462: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 463/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0298\n",
            "Epoch 00463: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 464/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0548\n",
            "Epoch 00464: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 465/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00465: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 466/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0352\n",
            "Epoch 00466: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 467/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0323\n",
            "Epoch 00467: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 468/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0355\n",
            "Epoch 00468: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 469/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00469: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 470/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00470: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 471/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0344\n",
            "Epoch 00471: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 472/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0244\n",
            "Epoch 00472: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 473/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0344\n",
            "Epoch 00473: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 474/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0470\n",
            "Epoch 00474: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 475/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0280\n",
            "Epoch 00475: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 476/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0429\n",
            "Epoch 00476: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 477/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0528\n",
            "Epoch 00477: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 478/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00478: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 479/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0220\n",
            "Epoch 00479: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 480/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0411\n",
            "Epoch 00480: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 481/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0343\n",
            "Epoch 00481: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 482/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00482: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 483/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0350\n",
            "Epoch 00483: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 484/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0253\n",
            "Epoch 00484: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 485/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0251\n",
            "Epoch 00485: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 486/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00486: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 487/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00487: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 488/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00488: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 489/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0211\n",
            "Epoch 00489: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 490/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00490: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 491/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0361\n",
            "Epoch 00491: val_loss did not improve from 0.03373\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 492/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0283\n",
            "Epoch 00492: val_loss improved from 0.03373 to 0.03371, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-492.hdf5\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 493/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0114\n",
            "Epoch 00493: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 494/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0629\n",
            "Epoch 00494: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 495/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 00495: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0341\n",
            "Epoch 496/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00496: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 497/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0271\n",
            "Epoch 00497: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 498/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00498: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 499/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 00499: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 500/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0307\n",
            "Epoch 00500: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 501/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0214\n",
            "Epoch 00501: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 502/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 00502: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 503/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00503: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 504/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0429\n",
            "Epoch 00504: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 505/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0285\n",
            "Epoch 00505: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 506/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0365\n",
            "Epoch 00506: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 507/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0316\n",
            "Epoch 00507: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 508/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0523\n",
            "Epoch 00508: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 509/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0276\n",
            "Epoch 00509: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 510/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0214\n",
            "Epoch 00510: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 511/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00511: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 512/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0297\n",
            "Epoch 00512: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 513/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0211\n",
            "Epoch 00513: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 514/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0438\n",
            "Epoch 00514: val_loss improved from 0.03371 to 0.03371, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-514.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 515/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00515: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 516/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0653\n",
            "Epoch 00516: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 517/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0476\n",
            "Epoch 00517: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 518/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00518: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 519/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00519: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 520/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00520: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 521/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 00521: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 522/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00522: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 523/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 00523: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 524/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00524: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 525/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00525: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 526/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0268\n",
            "Epoch 00526: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 527/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0504\n",
            "Epoch 00527: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 528/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0530\n",
            "Epoch 00528: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 529/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0322\n",
            "Epoch 00529: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 530/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0500\n",
            "Epoch 00530: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 531/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0242\n",
            "Epoch 00531: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 532/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00532: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 533/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00533: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 534/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0277\n",
            "Epoch 00534: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 535/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0140\n",
            "Epoch 00535: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 536/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00536: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 537/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0337\n",
            "Epoch 00537: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 538/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00538: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0340\n",
            "Epoch 539/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0369\n",
            "Epoch 00539: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 540/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0224\n",
            "Epoch 00540: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 541/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0645\n",
            "Epoch 00541: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 542/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0263\n",
            "Epoch 00542: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 543/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00543: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 544/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00544: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 545/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0564\n",
            "Epoch 00545: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 546/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0293\n",
            "Epoch 00546: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 547/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0433\n",
            "Epoch 00547: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 548/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0711\n",
            "Epoch 00548: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 549/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 00549: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 550/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0245\n",
            "Epoch 00550: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 551/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0441\n",
            "Epoch 00551: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 552/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0479\n",
            "Epoch 00552: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 553/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0429\n",
            "Epoch 00553: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 554/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0456\n",
            "Epoch 00554: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0341\n",
            "Epoch 555/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0156\n",
            "Epoch 00555: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 556/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0354\n",
            "Epoch 00556: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 557/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0234\n",
            "Epoch 00557: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 558/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 00558: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 559/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00559: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 560/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0251\n",
            "Epoch 00560: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 561/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0353\n",
            "Epoch 00561: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 562/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00562: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 563/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0267\n",
            "Epoch 00563: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 564/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00564: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 565/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0637\n",
            "Epoch 00565: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 566/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00566: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 567/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0224\n",
            "Epoch 00567: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 568/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0351\n",
            "Epoch 00568: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 569/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0377\n",
            "Epoch 00569: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 570/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0310\n",
            "Epoch 00570: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 571/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00571: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 572/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00572: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 573/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0277\n",
            "Epoch 00573: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 574/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0532\n",
            "Epoch 00574: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 575/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0923\n",
            "Epoch 00575: val_loss did not improve from 0.03371\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 576/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0283\n",
            "Epoch 00576: val_loss improved from 0.03371 to 0.03369, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-576.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 577/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 00577: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 578/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00578: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 579/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00579: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 580/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0479\n",
            "Epoch 00580: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 581/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0405\n",
            "Epoch 00581: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 582/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0410\n",
            "Epoch 00582: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0342\n",
            "Epoch 583/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0326\n",
            "Epoch 00583: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 584/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00584: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 585/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0297\n",
            "Epoch 00585: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 586/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00586: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 587/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0330\n",
            "Epoch 00587: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 588/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0345\n",
            "Epoch 00588: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 589/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0522\n",
            "Epoch 00589: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 590/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00590: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 591/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00591: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 592/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00592: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 593/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00593: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 594/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0393\n",
            "Epoch 00594: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 595/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00595: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 596/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00596: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 597/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0309\n",
            "Epoch 00597: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 598/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00598: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 599/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00599: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0337\n",
            "Epoch 600/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0316\n",
            "Epoch 00600: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 601/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0724\n",
            "Epoch 00601: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 602/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00602: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 603/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00603: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 604/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0406\n",
            "Epoch 00604: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 605/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0159\n",
            "Epoch 00605: val_loss improved from 0.03369 to 0.03369, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-605.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 606/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0335\n",
            "Epoch 00606: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 607/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00607: val_loss did not improve from 0.03369\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 608/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00608: val_loss improved from 0.03369 to 0.03366, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-608.hdf5\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 609/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00609: val_loss did not improve from 0.03366\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 610/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0377\n",
            "Epoch 00610: val_loss did not improve from 0.03366\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 611/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0357\n",
            "Epoch 00611: val_loss did not improve from 0.03366\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 612/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0156\n",
            "Epoch 00612: val_loss improved from 0.03366 to 0.03364, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-612.hdf5\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 613/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0261\n",
            "Epoch 00613: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 614/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0369\n",
            "Epoch 00614: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 615/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00615: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 616/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0324\n",
            "Epoch 00616: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 617/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0287\n",
            "Epoch 00617: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 618/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0620\n",
            "Epoch 00618: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 619/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0104\n",
            "Epoch 00619: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 620/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00620: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 621/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00621: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 622/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0364\n",
            "Epoch 00622: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 623/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0250\n",
            "Epoch 00623: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 624/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00624: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 625/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00625: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 626/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 00626: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 627/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0407\n",
            "Epoch 00627: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 628/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0358\n",
            "Epoch 00628: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 629/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0350\n",
            "Epoch 00629: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 630/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00630: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 631/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0373\n",
            "Epoch 00631: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 632/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0225\n",
            "Epoch 00632: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 633/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00633: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 634/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0169\n",
            "Epoch 00634: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 635/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00635: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 636/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00636: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 637/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0381\n",
            "Epoch 00637: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 638/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00638: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 639/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0328\n",
            "Epoch 00639: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 640/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0742\n",
            "Epoch 00640: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 641/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0329\n",
            "Epoch 00641: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 642/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0243\n",
            "Epoch 00642: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 643/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0306\n",
            "Epoch 00643: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 644/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0438\n",
            "Epoch 00644: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 645/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0479\n",
            "Epoch 00645: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 646/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00646: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 647/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0223\n",
            "Epoch 00647: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 648/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00648: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 649/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00649: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 650/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00650: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 651/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 00651: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 652/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00652: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 653/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0686\n",
            "Epoch 00653: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 654/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0406\n",
            "Epoch 00654: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 655/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0232\n",
            "Epoch 00655: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 656/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0249\n",
            "Epoch 00656: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 657/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 00657: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 658/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0234\n",
            "Epoch 00658: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 659/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0128\n",
            "Epoch 00659: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 660/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 00660: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 661/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00661: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 662/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00662: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 663/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0285\n",
            "Epoch 00663: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 664/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 00664: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0340\n",
            "Epoch 665/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 00665: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 666/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00666: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 667/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0247\n",
            "Epoch 00667: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 668/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00668: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 669/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0159\n",
            "Epoch 00669: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 670/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0451\n",
            "Epoch 00670: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 671/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0399\n",
            "Epoch 00671: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 672/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00672: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 673/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00673: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 674/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00674: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 675/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00675: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 676/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0382\n",
            "Epoch 00676: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 677/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 00677: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 678/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0594\n",
            "Epoch 00678: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 679/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 00679: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 680/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0160\n",
            "Epoch 00680: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 681/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 00681: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 682/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00682: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 683/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0568\n",
            "Epoch 00683: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 684/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00684: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 685/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0315\n",
            "Epoch 00685: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 686/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00686: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 687/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0388\n",
            "Epoch 00687: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 688/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0376\n",
            "Epoch 00688: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 689/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00689: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 690/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0653\n",
            "Epoch 00690: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 691/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0710\n",
            "Epoch 00691: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 692/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00692: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 693/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00693: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 694/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00694: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 695/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0470\n",
            "Epoch 00695: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 696/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0507\n",
            "Epoch 00696: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 697/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00697: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 698/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0576\n",
            "Epoch 00698: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 699/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0154\n",
            "Epoch 00699: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 700/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0422\n",
            "Epoch 00700: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 701/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00701: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 702/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0339\n",
            "Epoch 00702: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 703/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0218\n",
            "Epoch 00703: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 704/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0728\n",
            "Epoch 00704: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 705/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0135\n",
            "Epoch 00705: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 706/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00706: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 707/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00707: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 708/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00708: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 709/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00709: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 710/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0330\n",
            "Epoch 00710: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 711/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0364\n",
            "Epoch 00711: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 712/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0228\n",
            "Epoch 00712: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 713/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0201\n",
            "Epoch 00713: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 714/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0358\n",
            "Epoch 00714: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 715/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00715: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 716/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0313\n",
            "Epoch 00716: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 717/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0345\n",
            "Epoch 00717: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 718/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0231\n",
            "Epoch 00718: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 719/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00719: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 720/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0377\n",
            "Epoch 00720: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 721/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0289\n",
            "Epoch 00721: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 722/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00722: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 723/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0327\n",
            "Epoch 00723: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 0.0338\n",
            "Epoch 724/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0275\n",
            "Epoch 00724: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 725/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00725: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 726/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00726: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 727/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00727: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 728/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00728: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 729/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00729: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 730/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 00730: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 731/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00731: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 732/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00732: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 733/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0157\n",
            "Epoch 00733: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 734/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00734: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 735/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0359\n",
            "Epoch 00735: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 736/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0516\n",
            "Epoch 00736: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 737/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0329\n",
            "Epoch 00737: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 738/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0585\n",
            "Epoch 00738: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 739/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 00739: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 740/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0316\n",
            "Epoch 00740: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 741/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0628\n",
            "Epoch 00741: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 742/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0468\n",
            "Epoch 00742: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 743/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00743: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 744/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00744: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 745/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00745: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 746/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00746: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 747/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00747: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 748/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0333\n",
            "Epoch 00748: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 749/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0319\n",
            "Epoch 00749: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 750/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0390\n",
            "Epoch 00750: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 751/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0448\n",
            "Epoch 00751: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0339\n",
            "Epoch 752/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00752: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0336\n",
            "Epoch 753/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00753: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 754/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0532\n",
            "Epoch 00754: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 755/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0663\n",
            "Epoch 00755: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 756/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00756: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 757/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 00757: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 758/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0466\n",
            "Epoch 00758: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 759/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0430\n",
            "Epoch 00759: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 760/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0166\n",
            "Epoch 00760: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 761/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00761: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 762/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00762: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 763/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0267\n",
            "Epoch 00763: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 764/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00764: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 765/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0279\n",
            "Epoch 00765: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0340\n",
            "Epoch 766/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0454\n",
            "Epoch 00766: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 767/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0066\n",
            "Epoch 00767: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 768/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0363\n",
            "Epoch 00768: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 769/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00769: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 770/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0247\n",
            "Epoch 00770: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 771/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0159\n",
            "Epoch 00771: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 772/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0513\n",
            "Epoch 00772: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 773/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0235\n",
            "Epoch 00773: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 774/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0405\n",
            "Epoch 00774: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 775/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00775: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 776/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0106\n",
            "Epoch 00776: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 777/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00777: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 778/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 00778: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 779/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 00779: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 780/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0417\n",
            "Epoch 00780: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 781/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00781: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 782/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0218\n",
            "Epoch 00782: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 783/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00783: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 784/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0138\n",
            "Epoch 00784: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 785/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 00785: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 786/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00786: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 787/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0286\n",
            "Epoch 00787: val_loss did not improve from 0.03364\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0341\n",
            "Epoch 788/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0444\n",
            "Epoch 00788: val_loss improved from 0.03364 to 0.03361, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-788.hdf5\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 789/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0333\n",
            "Epoch 00789: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 790/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00790: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 791/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0220\n",
            "Epoch 00791: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 792/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0288\n",
            "Epoch 00792: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 793/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00793: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 794/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0275\n",
            "Epoch 00794: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 795/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0366\n",
            "Epoch 00795: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 796/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0281\n",
            "Epoch 00796: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 797/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00797: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 798/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0283\n",
            "Epoch 00798: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 799/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00799: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 800/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0282\n",
            "Epoch 00800: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 801/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0295\n",
            "Epoch 00801: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 802/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0309\n",
            "Epoch 00802: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 803/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0584\n",
            "Epoch 00803: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 804/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0279\n",
            "Epoch 00804: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 805/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0438\n",
            "Epoch 00805: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 806/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0196\n",
            "Epoch 00806: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 807/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00807: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 808/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0746\n",
            "Epoch 00808: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 809/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0155\n",
            "Epoch 00809: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 810/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00810: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 811/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00811: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 812/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0123\n",
            "Epoch 00812: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 813/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00813: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 814/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0386\n",
            "Epoch 00814: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 815/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0255\n",
            "Epoch 00815: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 816/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00816: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 817/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00817: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 818/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00818: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 819/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0250\n",
            "Epoch 00819: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 820/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0350\n",
            "Epoch 00820: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 821/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00821: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 822/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0307\n",
            "Epoch 00822: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 823/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0230\n",
            "Epoch 00823: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 824/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0297\n",
            "Epoch 00824: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 825/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0166\n",
            "Epoch 00825: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 826/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0203\n",
            "Epoch 00826: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 827/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0218\n",
            "Epoch 00827: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 828/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0363\n",
            "Epoch 00828: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 829/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0251\n",
            "Epoch 00829: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 830/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0290\n",
            "Epoch 00830: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 831/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0409\n",
            "Epoch 00831: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 832/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0263\n",
            "Epoch 00832: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 833/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0676\n",
            "Epoch 00833: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 834/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00834: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 835/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 00835: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 836/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0619\n",
            "Epoch 00836: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 837/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00837: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 838/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00838: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 839/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0255\n",
            "Epoch 00839: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 840/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 00840: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 841/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0245\n",
            "Epoch 00841: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 842/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 00842: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 843/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0236\n",
            "Epoch 00843: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 844/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00844: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 845/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0391\n",
            "Epoch 00845: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 846/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0341\n",
            "Epoch 00846: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 847/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00847: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 848/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00848: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 849/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0221\n",
            "Epoch 00849: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0340\n",
            "Epoch 850/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0575\n",
            "Epoch 00850: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 851/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 00851: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 852/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00852: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 853/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00853: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 854/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00854: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 855/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00855: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 856/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0532\n",
            "Epoch 00856: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 857/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0269\n",
            "Epoch 00857: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 858/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0422\n",
            "Epoch 00858: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 859/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00859: val_loss did not improve from 0.03361\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0340\n",
            "Epoch 860/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0231\n",
            "Epoch 00860: val_loss improved from 0.03361 to 0.03360, saving model to /content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-860.hdf5\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 861/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0262\n",
            "Epoch 00861: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 862/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0588\n",
            "Epoch 00862: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 863/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0576\n",
            "Epoch 00863: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 864/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0561\n",
            "Epoch 00864: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 865/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0253\n",
            "Epoch 00865: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 866/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0417\n",
            "Epoch 00866: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 867/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0310\n",
            "Epoch 00867: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0337\n",
            "Epoch 868/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0451\n",
            "Epoch 00868: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 869/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00869: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 870/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0309\n",
            "Epoch 00870: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 871/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 00871: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 872/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0399\n",
            "Epoch 00872: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 873/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0244\n",
            "Epoch 00873: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 874/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0125\n",
            "Epoch 00874: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 875/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0332\n",
            "Epoch 00875: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 876/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0572\n",
            "Epoch 00876: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 877/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0309\n",
            "Epoch 00877: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 878/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00878: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 879/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0593\n",
            "Epoch 00879: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 880/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00880: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 881/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00881: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 882/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0397\n",
            "Epoch 00882: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 883/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0161\n",
            "Epoch 00883: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0339\n",
            "Epoch 884/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 00884: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 885/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0302\n",
            "Epoch 00885: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 886/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0470\n",
            "Epoch 00886: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 887/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0404\n",
            "Epoch 00887: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 888/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00888: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 889/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00889: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0336\n",
            "Epoch 890/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00890: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 891/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0679\n",
            "Epoch 00891: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 892/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0280\n",
            "Epoch 00892: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 893/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0136\n",
            "Epoch 00893: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 894/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0270\n",
            "Epoch 00894: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 895/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 00895: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 896/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0205\n",
            "Epoch 00896: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 897/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00897: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 898/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0304\n",
            "Epoch 00898: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 899/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0385\n",
            "Epoch 00899: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0324 - val_loss: 0.0337\n",
            "Epoch 900/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0319\n",
            "Epoch 00900: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 901/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0127\n",
            "Epoch 00901: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 902/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00902: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 903/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00903: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 904/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0377\n",
            "Epoch 00904: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 905/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0207\n",
            "Epoch 00905: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 906/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0449\n",
            "Epoch 00906: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 907/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0287\n",
            "Epoch 00907: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 908/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0196\n",
            "Epoch 00908: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 909/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00909: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 910/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0385\n",
            "Epoch 00910: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 911/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00911: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 912/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0258\n",
            "Epoch 00912: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 913/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0648\n",
            "Epoch 00913: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 914/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0470\n",
            "Epoch 00914: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 915/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0395\n",
            "Epoch 00915: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 916/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00916: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 917/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0218\n",
            "Epoch 00917: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 918/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0300\n",
            "Epoch 00918: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0336\n",
            "Epoch 919/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0255\n",
            "Epoch 00919: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 920/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0567\n",
            "Epoch 00920: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 921/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0316\n",
            "Epoch 00921: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 922/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0362\n",
            "Epoch 00922: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 923/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0113\n",
            "Epoch 00923: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 924/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 00924: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 925/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0434\n",
            "Epoch 00925: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 926/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0327\n",
            "Epoch 00926: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0337\n",
            "Epoch 927/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0250\n",
            "Epoch 00927: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 928/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0301\n",
            "Epoch 00928: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 929/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0474\n",
            "Epoch 00929: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 930/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00930: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 931/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00931: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 932/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00932: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 933/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0273\n",
            "Epoch 00933: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 934/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0294\n",
            "Epoch 00934: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0336\n",
            "Epoch 935/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0391\n",
            "Epoch 00935: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 936/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0497\n",
            "Epoch 00936: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 937/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0380\n",
            "Epoch 00937: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0339\n",
            "Epoch 938/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0312\n",
            "Epoch 00938: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 939/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00939: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 940/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0311\n",
            "Epoch 00940: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 941/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0268\n",
            "Epoch 00941: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 942/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00942: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 943/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00943: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 944/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0546\n",
            "Epoch 00944: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 945/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0264\n",
            "Epoch 00945: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 946/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00946: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 947/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0784\n",
            "Epoch 00947: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 948/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0703\n",
            "Epoch 00948: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 949/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00949: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 950/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0325\n",
            "Epoch 00950: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 951/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0238\n",
            "Epoch 00951: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 952/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00952: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 953/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0416\n",
            "Epoch 00953: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 954/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0135\n",
            "Epoch 00954: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 955/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0346\n",
            "Epoch 00955: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 956/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0547\n",
            "Epoch 00956: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0336\n",
            "Epoch 957/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0356\n",
            "Epoch 00957: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 958/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0130\n",
            "Epoch 00958: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 959/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0511\n",
            "Epoch 00959: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 960/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0320\n",
            "Epoch 00960: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 961/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0743\n",
            "Epoch 00961: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 962/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00962: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 963/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0260\n",
            "Epoch 00963: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 964/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0485\n",
            "Epoch 00964: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 965/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0415\n",
            "Epoch 00965: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 966/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0354\n",
            "Epoch 00966: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 967/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0408\n",
            "Epoch 00967: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 968/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00968: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 969/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00969: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 970/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00970: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 971/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0344\n",
            "Epoch 00971: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 972/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0334\n",
            "Epoch 00972: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 973/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0374\n",
            "Epoch 00973: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 974/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00974: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 975/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00975: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 976/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0343\n",
            "Epoch 00976: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 977/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00977: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 978/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0397\n",
            "Epoch 00978: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 979/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0399\n",
            "Epoch 00979: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 980/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00980: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0339\n",
            "Epoch 981/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0383\n",
            "Epoch 00981: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 982/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0257\n",
            "Epoch 00982: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 983/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0431\n",
            "Epoch 00983: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 984/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0380\n",
            "Epoch 00984: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 985/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0540\n",
            "Epoch 00985: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 986/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0392\n",
            "Epoch 00986: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0337\n",
            "Epoch 987/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00987: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 988/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00988: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 989/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0428\n",
            "Epoch 00989: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0337\n",
            "Epoch 990/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00990: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 991/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0215\n",
            "Epoch 00991: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0337\n",
            "Epoch 992/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00992: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 993/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00993: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0338\n",
            "Epoch 994/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0218\n",
            "Epoch 00994: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 995/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0219\n",
            "Epoch 00995: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0338\n",
            "Epoch 996/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0239\n",
            "Epoch 00996: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0337\n",
            "Epoch 997/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00997: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0338\n",
            "Epoch 998/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0129\n",
            "Epoch 00998: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0336\n",
            "Epoch 999/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0293\n",
            "Epoch 00999: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n",
            "Epoch 1000/1000\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 0.0298\n",
            "Epoch 01000: val_loss did not improve from 0.03360\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZGd-UcRxNoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIahfD-5BKvZ",
        "colab_type": "code",
        "outputId": "222c599e-a52e-4b89-e038-0b10436bcf08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "y_pred = yscaler.inverse_transform(model.predict(X_test)) \n",
        "y_pred_df = pd.DataFrame(y_pred*100, columns= out_cols)\n",
        "y_test_df = pd.DataFrame(yscaler.inverse_transform(y_test)*100, columns= out_cols)\n",
        "\n",
        "pd.concat([y_pred_df, y_test_df], axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1_day_return</th>\n",
              "      <th>3_day_return</th>\n",
              "      <th>5_day_return</th>\n",
              "      <th>1_day_return</th>\n",
              "      <th>3_day_return</th>\n",
              "      <th>5_day_return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.113509</td>\n",
              "      <td>0.284950</td>\n",
              "      <td>0.461296</td>\n",
              "      <td>2.971005</td>\n",
              "      <td>0.549524</td>\n",
              "      <td>1.608363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.122420</td>\n",
              "      <td>0.304328</td>\n",
              "      <td>0.482929</td>\n",
              "      <td>-1.217247</td>\n",
              "      <td>-1.662439</td>\n",
              "      <td>-0.737565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.094424</td>\n",
              "      <td>-0.122508</td>\n",
              "      <td>-0.118221</td>\n",
              "      <td>-0.171429</td>\n",
              "      <td>-1.371429</td>\n",
              "      <td>-3.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.334551</td>\n",
              "      <td>0.668192</td>\n",
              "      <td>0.990029</td>\n",
              "      <td>0.215850</td>\n",
              "      <td>0.370028</td>\n",
              "      <td>-1.171755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.024021</td>\n",
              "      <td>0.017168</td>\n",
              "      <td>0.075124</td>\n",
              "      <td>-1.773186</td>\n",
              "      <td>0.807304</td>\n",
              "      <td>2.162422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>-0.149266</td>\n",
              "      <td>-0.222720</td>\n",
              "      <td>-0.283199</td>\n",
              "      <td>-2.725968</td>\n",
              "      <td>-3.873745</td>\n",
              "      <td>-7.986609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.107616</td>\n",
              "      <td>0.246858</td>\n",
              "      <td>0.413186</td>\n",
              "      <td>0.356951</td>\n",
              "      <td>0.549641</td>\n",
              "      <td>0.353792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>-0.173376</td>\n",
              "      <td>-0.269972</td>\n",
              "      <td>-0.350380</td>\n",
              "      <td>0.143821</td>\n",
              "      <td>0.092193</td>\n",
              "      <td>-0.667478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.060553</td>\n",
              "      <td>0.181908</td>\n",
              "      <td>0.312488</td>\n",
              "      <td>1.013799</td>\n",
              "      <td>2.346757</td>\n",
              "      <td>2.816108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>-0.064774</td>\n",
              "      <td>-0.059341</td>\n",
              "      <td>-0.044055</td>\n",
              "      <td>0.417029</td>\n",
              "      <td>-0.434405</td>\n",
              "      <td>0.712424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     1_day_return  3_day_return  ...  3_day_return  5_day_return\n",
              "0        0.113509      0.284950  ...      0.549524      1.608363\n",
              "1        0.122420      0.304328  ...     -1.662439     -0.737565\n",
              "2       -0.094424     -0.122508  ...     -1.371429     -3.885714\n",
              "3        0.334551      0.668192  ...      0.370028     -1.171755\n",
              "4       -0.024021      0.017168  ...      0.807304      2.162422\n",
              "..            ...           ...  ...           ...           ...\n",
              "161     -0.149266     -0.222720  ...     -3.873745     -7.986609\n",
              "162      0.107616      0.246858  ...      0.549641      0.353792\n",
              "163     -0.173376     -0.269972  ...      0.092193     -0.667478\n",
              "164      0.060553      0.181908  ...      2.346757      2.816108\n",
              "165     -0.064774     -0.059341  ...     -0.434405      0.712424\n",
              "\n",
              "[166 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5do3GPzVb4mS",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a890acbf-3f21-40c9-9794-4e32bb0df9c2",
        "id": "gk8hiZe9hdvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "linreg = LinearRegression()\n",
        "sc = MinMaxScaler()\n",
        "\n",
        "\n",
        "pipe_linreg = Pipeline([('scaler', sc), ('clf', linreg)])\n",
        "pipe_linreg.fit(X_train[['bearish_score', 'bullish_score']], y_train['3_day_return'])\n",
        "pipe_linreg.score(X_train[['bearish_score', 'bullish_score']], y_train['3_day_return'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0007906095944075764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR0CCKF0iYja",
        "colab_type": "code",
        "outputId": "0f2cf307-de7d-462b-a8ea-617068c6296f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sc = StandardScaler()\n",
        "nn = MLPRegressor(hidden_layer_sizes=(5, 5, 3, 5, 5, 5), random_state=42)\n",
        "\n",
        "\n",
        "col = ['5_day_return']\n",
        "\n",
        "pipe_nn = Pipeline([('scaler', sc), ('clf', nn)])\n",
        "pipe_nn.fit(X_train, y_train['5_day_return'])\n",
        "pipe_nn.score(X_train, y_train['5_day_return'])\n",
        "\n",
        "y_pred_train = pipe_nn.predict(X_train)\n",
        "y_pred_test = pipe_nn.predict(X_test)\n",
        "metrics.mean_squared_error()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.02616142847881719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jVsO5NFlakL",
        "colab_type": "code",
        "outputId": "fefa75e3-aecc-47fb-b554-a0fbb6dbb0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "pd.DataFrame({'y_pred': pipe_nn.predict(X_train), \n",
        "              'y_true': y_train['5_day_return']})*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>y_pred</th>\n",
              "      <th>y_true</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>created_at</th>\n",
              "      <th>ticker</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-08-23</th>\n",
              "      <th>DIA</th>\n",
              "      <td>1.813302</td>\n",
              "      <td>3.043427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-15</th>\n",
              "      <th>MSFT</th>\n",
              "      <td>1.519598</td>\n",
              "      <td>2.169384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-12</th>\n",
              "      <th>MSFT</th>\n",
              "      <td>1.256726</td>\n",
              "      <td>1.611851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-10-19</th>\n",
              "      <th>DIA</th>\n",
              "      <td>0.290459</td>\n",
              "      <td>0.969390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-20</th>\n",
              "      <th>VIX</th>\n",
              "      <td>0.340432</td>\n",
              "      <td>5.238345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-10-19</th>\n",
              "      <th>VIX</th>\n",
              "      <td>0.284142</td>\n",
              "      <td>-3.352941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-01</th>\n",
              "      <th>DIA</th>\n",
              "      <td>1.042885</td>\n",
              "      <td>1.364601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-28</th>\n",
              "      <th>TWTR</th>\n",
              "      <td>0.231327</td>\n",
              "      <td>1.592257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-31</th>\n",
              "      <th>GOOGL</th>\n",
              "      <td>0.358130</td>\n",
              "      <td>4.901485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-21</th>\n",
              "      <th>AMD</th>\n",
              "      <td>0.531862</td>\n",
              "      <td>-1.018609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>735 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     y_pred    y_true\n",
              "created_at ticker                    \n",
              "2019-08-23 DIA     1.813302  3.043427\n",
              "2020-01-15 MSFT    1.519598  2.169384\n",
              "2019-12-12 MSFT    1.256726  1.611851\n",
              "2019-10-19 DIA     0.290459  0.969390\n",
              "2019-09-20 VIX     0.340432  5.238345\n",
              "...                     ...       ...\n",
              "2019-10-19 VIX     0.284142 -3.352941\n",
              "2019-11-01 DIA     1.042885  1.364601\n",
              "2019-12-28 TWTR    0.231327  1.592257\n",
              "2019-12-31 GOOGL   0.358130  4.901485\n",
              "2020-01-21 AMD     0.531862 -1.018609\n",
              "\n",
              "[735 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uObQjiTAmpk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics.mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysGfCQS191ec",
        "colab_type": "text"
      },
      "source": [
        "# Fetch Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWRdi5Fl_g99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, X_test, yscaler):\n",
        "  y_pred = yscaler.inverse_transform(model.predict(X_test)) \n",
        "  y_pred_df = pd.DataFrame(y_pred*100, columns= out_cols)\n",
        "  y_test_df = pd.DataFrame(yscaler.inverse_transform(y_test)*100, columns= out_cols)\n",
        "  df = pd.concat([X_test.reset_index(), y_pred_df, y_test_df], axis=1)\n",
        "  df.set_index(['created_at', 'ticker'])\n",
        "  return df\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIou_569dUY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path= Path('/content/drive/My Drive/logs/finpred_5_/weights-improvement-0.03-12.hdf5')\n",
        "model = models.load_model(model_path)\n",
        "predictions_file = Path('FinancialPrediction/predictions.pkl')\n",
        "predict(model, X_test, yscaler).to_pickle(predictions_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5bdmrDyJh6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_df = pd.read_pickle(predictions_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpX_dWkJ1zD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_values(cols, ticker=None, rdate=None):\n",
        "    if not ticker and not rdate:\n",
        "      raise ValueError('at least one of ticker or rdate should be provided')\n",
        "    df_filter = [True]*len(predictions_df)\n",
        "    if ticker:\n",
        "      df_filter = predictions_df['ticker'] == ticker\n",
        "    if rdate:\n",
        "      df_filter = (predictions_df['created_at'] == rdate) & df_filter\n",
        "    df_subset = predictions_df.loc[df_filter, cols]\n",
        "    return df_subset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpiE5RKGJ6tW",
        "colab_type": "code",
        "outputId": "7b0ac8ae-ff76-4aa8-fb38-47d7525dc55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cols = ['1_day_return', '3_day_return']\n",
        "ticker='AAPL'\n",
        "return_list = get_values(cols=cols, ticker=ticker, rdate=date(2020,1,13)).values.flatten().tolist()\n",
        "col_names_str = \", \".join([c[:-6].replace('_', ' ').rstrip() for c in cols])\n",
        "return_format = lambda r: str(math.ceil(r * 100) / 100.0) + '%'\n",
        "returns_str = ', '.join([return_format(r) for r in return_list])\n",
        "print(f'The {col_names_str} returns for {ticker} are {returns_str}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 1 day, 3 day returns for AAPL are -2.13%, -3.13%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}