{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Analysis of new media on stock returns</center><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Team â€“ Group 13\n",
    "- Vidur  Puliani          A0198492L\n",
    "- Lei Xiaowen             A0198449H\n",
    "- Donal Ngo Jin Ze        A0198487A\n",
    "- Wilson Lum Kok Keong    A0198478A\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To scrape Twitter data for the 10 technology stosks name as follow :\n",
    "- Apple = NASDAQ: AAPL\n",
    "- Google = NASDAQ: GOOGL\n",
    "- Facebook = NASDAQ: FB\n",
    "- Amazon = NASDAQ: AMZN\n",
    "- Microsoft = NASDAQ: MSFT\n",
    "- Alibaba = NYSE: BABA\n",
    "- AMD = NASDAQ: AMD\n",
    "- Intel = NASDAQ: INTC\n",
    "- Tesla = NASDAQ: TSLA\n",
    "- DOw Jones = DIA\n",
    "- SPider = NSADAQ: SPY\n",
    "- Tweeter = TWTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Authenication credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date\n",
    "import tweepy\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Twitter account API credential details\n",
    "consumer_key = \"L8KaOD3tPINfmV74ZvQv7onCV\" \n",
    "consumer_secret = \"HPvvTrEVPtr7yTsOnSSOZ2XOIHxa4Kk0yrTKyFosgdPLO927i1\"\n",
    "access_token = \"1201691382752014337-afzYJEq3tQNNwFg4TVnh0yToZX6Cnj\"\n",
    "access_token_secret = \"wm8zwYtlc47tWnl8Ah8nzaSE7qDAeUpF1wUymC2qpVMJ7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.careated_at)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = ['$AAPL','$goog','$FB ','$amzn','$MSFT','$baba','$AMD', '$intc','$tsla','$dia','$spy','$twtr','$vix']\n",
    "\n",
    "maxTweets    = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect tweets per stocks\n",
    "# Favorite_count => A tweet that is favorited\n",
    "#                   The favorite_count provides the number of times the tweet has been favorited. \n",
    "#                   In the case of a retweet, favorite_count is the favorite count of the source tweet\n",
    "# retweet_count  => A Retweet is a re-posting of a Tweet. Twitter's Retweet feature helps you and others quickly share that  \n",
    "#                   Tweet with all of your followers. You can Retweet your own Tweets or Tweets from someone else. \n",
    "#                   Sometimes people type \"RT\" at the beginning of a Tweet to indicate that they are re-posting \n",
    "#                   someone else's content.\n",
    "\n",
    "def countdown(t, step=1, msg='sleeping'):  # in seconds\n",
    "    pad_str = ' ' * len('%d' % step)\n",
    "    for i in range(t, 0, -step):\n",
    "        print('\\r%s for the next %d seconds %s\\r' % (msg, i, pad_str),end='\\r')\n",
    "        time.sleep(step)\n",
    "    print('Done %s for %d seconds! ' % (msg, t))\n",
    "    \n",
    "def collect_tweets(search_query):\n",
    "    rest         = 900 # if rating error wait for 15 mins - Twitter limit\n",
    "    my_tweets    = []\n",
    "    maxTweets    = 100000 # Some arbitrary large number\n",
    "    tweetsPerQry = 100   # this is the max the API permits\n",
    "    \n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = None\n",
    "    \n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1\n",
    "    \n",
    "    tweetCount = 0\n",
    "    \n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "    print (\"Collecting tweets for   :\" + str(search_query))\n",
    "    \n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=search_query, count=tweetsPerQry,lang='en',include_entities=True)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_queryy, count=tweetsPerQry,lang='en',since_id=sinceId,\n",
    "                                            include_entities=True)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=search_query, count=tweetsPerQry,lang='en',max_id=str(max_id - 1),\n",
    "                                            include_entities=True)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query, count=tweetsPerQry,lang='en',max_id=str(max_id - 1),\n",
    "                                            include_entities=True,since_id=sinceId)\n",
    "       \n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            \n",
    "            for tweet in new_tweets:\n",
    "                my_tweets.append([tweet.created_at, tweet.text,tweet.favorite_count,tweet.retweet_count,search_query])\n",
    "            \n",
    "            tweetCount += len(new_tweets)\n",
    "            \n",
    "            if (tweetCount % 1000==0):\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "         \n",
    "            max_id = new_tweets[-1].id\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            print(\"Sleeping ... for error ... {0}s ...\".format(rest))\n",
    "            \n",
    "            countdown(rest)\n",
    "            \n",
    "            if (tweetCount % 1000==0):\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "\n",
    "    print (\"Downloaded {0} tweets\".format(tweetCount))\n",
    "    \n",
    "    return my_tweets\n",
    "\n",
    "# Preprocess the tweets\n",
    "def pre_process_tweets(my_tweets,search_query):\n",
    "    processed_tweets = []\n",
    "    tweets_processed =my_tweets\n",
    "    print (\"Preprocessing texts for :\" + str(search_query))\n",
    "    \n",
    "    for tweet in range(0, len(my_tweets)):  \n",
    "    \n",
    "        # Remove all the http links\n",
    "        processed_tweet = re.sub(r'http\\S+', '', str(my_tweets[tweet][1]))\n",
    "        \n",
    "        # Remove all the special characters\n",
    "        processed_tweet = re.sub(r'\\W', ' ', processed_tweet)\n",
    " \n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "        # Removing prefixed 'b'\n",
    "        processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    " \n",
    "        # Converting to Lowercase\n",
    "        processed_tweet = processed_tweet.lower()\n",
    "        \n",
    "        tweets_processed[tweet][1] = processed_tweet\n",
    "       \n",
    "    return tweets_processed\n",
    "\n",
    "\n",
    "def Remove(my_tweets,search_query): \n",
    "    \n",
    "    print (\"Removing dulpicates texts for :\" + str(search_query))\n",
    "    text = []\n",
    "    new_list = []\n",
    "    for tweet in range(0, (len(my_tweets))):  \n",
    "        if tweet < (len(my_tweets)-1):\n",
    "            if my_tweets[tweet][1] not in text:\n",
    "                text.append(my_tweets[tweet][1])\n",
    "                new_list.append(my_tweets[tweet]) \n",
    "            \n",
    "    return new_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Tweets and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$AAPL\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 4000 tweets\n",
      "Downloaded 5000 tweets\n",
      "No more tweets found\n",
      "Downloaded 16137 tweets\n",
      "Preprocessing texts for :$AAPL\n",
      "Removing dulpicates texts for :$AAPL\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$goog\n",
      "Downloaded 1000 tweets\n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "Downloaded 2000 tweets\n",
      "No more tweets found\n",
      "Downloaded 2434 tweets\n",
      "Preprocessing texts for :$goog\n",
      "Removing dulpicates texts for :$goog\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$FB \n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "No more tweets found\n",
      "Downloaded 6489 tweets\n",
      "Preprocessing texts for :$FB \n",
      "Removing dulpicates texts for :$FB \n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$amzn\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "No more tweets found\n",
      "Downloaded 9352 tweets\n",
      "Preprocessing texts for :$amzn\n",
      "Removing dulpicates texts for :$amzn\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$MSFT\n",
      "Downloaded 1000 tweets\n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 4000 tweets\n",
      "Downloaded 5000 tweets\n",
      "Downloaded 6000 tweets\n",
      "Downloaded 7000 tweets\n",
      "Downloaded 8000 tweets\n",
      "No more tweets found\n",
      "Downloaded 8338 tweets\n",
      "Preprocessing texts for :$MSFT\n",
      "Removing dulpicates texts for :$MSFT\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$baba\n",
      "Downloaded 1000 tweets\n",
      "No more tweets found\n",
      "Downloaded 2775 tweets\n",
      "Preprocessing texts for :$baba\n",
      "Removing dulpicates texts for :$baba\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$AMD\n",
      "Downloaded 1000 tweets\n",
      "No more tweets found\n",
      "Downloaded 4007 tweets\n",
      "Preprocessing texts for :$AMD\n",
      "Removing dulpicates texts for :$AMD\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$intc\n",
      "Downloaded 1000 tweets\n",
      "No more tweets found\n",
      "Downloaded 1220 tweets\n",
      "Preprocessing texts for :$intc\n",
      "Removing dulpicates texts for :$intc\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$tsla\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 4000 tweets\n",
      "Downloaded 5000 tweets\n",
      "Downloaded 6000 tweets\n",
      "Downloaded 7000 tweets\n",
      "Downloaded 8000 tweets\n",
      "Downloaded 9000 tweets\n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "No more tweets found\n",
      "Downloaded 32467 tweets\n",
      "Preprocessing texts for :$tsla\n",
      "Removing dulpicates texts for :$tsla\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$dia\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 4000 tweets\n",
      "No more tweets found\n",
      "Downloaded 4369 tweets\n",
      "Preprocessing texts for :$dia\n",
      "Removing dulpicates texts for :$dia\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$spy\n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 4000 tweets\n",
      "Downloaded 5000 tweets\n",
      "Downloaded 6000 tweets\n",
      "Downloaded 7000 tweets\n",
      "Downloaded 8000 tweets\n",
      "Downloaded 9000 tweets\n",
      "Downloaded 10000 tweets\n",
      "Downloaded 11000 tweets\n",
      "Downloaded 12000 tweets\n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "some error : [{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "Sleeping ... for error ... 900s ...\n",
      "sleeping for the next 1 seconds    Done sleeping for 900 seconds! \n",
      "No more tweets found\n",
      "Downloaded 39353 tweets\n",
      "Preprocessing texts for :$spy\n",
      "Removing dulpicates texts for :$spy\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$twtr\n",
      "Downloaded 1000 tweets\n",
      "No more tweets found\n",
      "Downloaded 3380 tweets\n",
      "Preprocessing texts for :$twtr\n",
      "Removing dulpicates texts for :$twtr\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Downloading max 100000 tweets\n",
      "Collecting tweets for   :$vix\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 4000 tweets\n",
      "Downloaded 5000 tweets\n",
      "Downloaded 6000 tweets\n",
      "Downloaded 7000 tweets\n",
      "Downloaded 8000 tweets\n",
      "Downloaded 9000 tweets\n",
      "No more tweets found\n",
      "Downloaded 9081 tweets\n",
      "Preprocessing texts for :$vix\n",
      "Removing dulpicates texts for :$vix\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# List of stocks to collect for tweets\n",
    "my_tweets_df = [ [0] * maxTweets for _ in range(len(search_query))]\n",
    "header = ['Date', 'Text', 'favorite_count', 'retweet_count','ticker']\n",
    "\n",
    "for i in range (0,len(search_query)):\n",
    "    my_tweets       = collect_tweets(search_query[i])\n",
    "    my_tweets_1     = pre_process_tweets(my_tweets,search_query[i])\n",
    "    my_tweets_2     = Remove(my_tweets_1,search_query[i])\n",
    "    my_tweets_df[i] = pd.DataFrame(my_tweets_2)\n",
    "    \n",
    "    if not my_tweets_df[i].empty:\n",
    "        my_tweets_df[i].columns = header\n",
    "         \n",
    "    print(\"\\n------------------------------------------\\n\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets for $AAPL is 10438\n",
      "Number of tweets for $goog is 1763\n",
      "Number of tweets for $FB  is 4103\n",
      "Number of tweets for $amzn is 5923\n",
      "Number of tweets for $MSFT is 5163\n",
      "Number of tweets for $baba is 1918\n",
      "Number of tweets for $AMD is 3028\n",
      "Number of tweets for $intc is 916\n",
      "Number of tweets for $tsla is 18574\n",
      "Number of tweets for $dia is 3035\n",
      "Number of tweets for $spy is 30253\n",
      "Number of tweets for $twtr is 2497\n",
      "Number of tweets for $vix is 5502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('$spy', 30253),\n",
       " ('$tsla', 18574),\n",
       " ('$AAPL', 10438),\n",
       " ('$amzn', 5923),\n",
       " ('$vix', 5502),\n",
       " ('$MSFT', 5163),\n",
       " ('$FB ', 4103),\n",
       " ('$dia', 3035),\n",
       " ('$AMD', 3028),\n",
       " ('$twtr', 2497),\n",
       " ('$baba', 1918),\n",
       " ('$goog', 1763),\n",
       " ('$intc', 916)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stock_counter = Counter()\n",
    "\n",
    "for i in range(0,len(my_tweets_df)):\n",
    "    Stock_counter[search_query[i]] = len(my_tweets_df[i])\n",
    "    print(\"Number of tweets for \" + search_query[i] + \" is \" + str(len(my_tweets_df[i])))\n",
    "\n",
    "Stock_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#*******************************************************************************\n",
    "#                         Save data\n",
    "#*******************************************************************************\n",
    "\n",
    "# Month abbreviation, day and year\n",
    "today = date.today()\n",
    "d = today.strftime(\"%b-%d-%Y\")\n",
    "\n",
    "# Create a new excel workbook\n",
    "writer = pd.ExcelWriter('data/stock_tweets_' + d + '.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for i in range (0,len(search_query)):\n",
    "    my_tweets_df[i].to_excel(writer, sheet_name= search_query[i], index=False)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
